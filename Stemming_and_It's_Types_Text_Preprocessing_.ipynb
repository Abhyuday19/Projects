{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQA8tSr4rH4FgoVN/9tRN8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhyuday19/Projects/blob/main/Stemming_and_It's_Types_Text_Preprocessing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "cpcBazbExPHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Porter Stemmer"
      ],
      "metadata": {
        "id": "RmfVS1bix283"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['Eating' , 'Eat' , 'Eaten' , \"History\" ,'Going' , 'Gone' , 'Go' , 'Playing' , 'Played' , 'Play', 'Finalized' , 'Final', 'Finally' ]\n",
        "from nltk.stem import PorterStemmer ## it has disadvantage as it does not properly gives the stemm(root) word of the given word sometimes like in history word it gives hitori.\n",
        "Stemming = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"----->\" +Stemming.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJt8ZrOxx8Fk",
        "outputId": "cc0b0734-e709-48ab-fbd2-757950bc1b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating----->eat\n",
            "Eat----->eat\n",
            "Eaten----->eaten\n",
            "History----->histori\n",
            "Going----->go\n",
            "Gone----->gone\n",
            "Go----->go\n",
            "Playing----->play\n",
            "Played----->play\n",
            "Play----->play\n",
            "Finalized----->final\n",
            "Final----->final\n",
            "Finally----->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Regexp Stemmer"
      ],
      "metadata": {
        "id": "0E2G75mB4n5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpStemmer\n",
        "regexp_1 = RegexpStemmer('(ing|s|able)$' , min=3)\n",
        "regexp_1.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7yiJPeZpy5o7",
        "outputId": "1893f6d4-bb03-4c62-bfdd-b8a9fa9273d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. SnowBall Stemmer"
      ],
      "metadata": {
        "id": "06z2e_Ex3wSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import SnowballStemmer\n",
        "stemmer_12 = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "wR991-AG41nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+'------>'+stemmer_12.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOrL_KdM5Chj",
        "outputId": "666da8ce-9844-4ef8-f4bc-6707fa382415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating------>eat\n",
            "Eat------>eat\n",
            "Eaten------>eaten\n",
            "History------>histori\n",
            "Going------>go\n",
            "Gone------>gone\n",
            "Go------>go\n",
            "Playing------>play\n",
            "Played------>play\n",
            "Play------>play\n",
            "Finalized------>final\n",
            "Final------>final\n",
            "Finally------>final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_12.stem('fairly') , stemmer_12.stem('sportingly') ## here in this case the Snowball stemmer gives better results(ouput) than the Poter Stemmer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmH8Gb_D5N5H",
        "outputId": "91ba1517-22dd-47c0-de40-71ffd8ac80ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "kuk6jiU_567d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8oQHWK5KR2T",
        "outputId": "82be0183-034b-48f3-9cc7-c8995d316344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lematizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Z7qtVwuDKWqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lematizer.lemmatize('going' , pos = 'a')  # here there are two arguments , one is word , and second is the position tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T5L9KegBKyfB",
        "outputId": "b8813858-9529-4fcd-8480-7d3eaeefe087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here in upwards the position tag will have the r as adverb , v as verb ,  n as noun , a - adjective"
      ],
      "metadata": {
        "id": "qIJ2IHzUK8oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['Eating' , 'Eat' , 'Eaten' , \"History\" ,'Going' , 'Gone' , 'Go' , 'Playing' , 'Played' , 'Plays', 'Finalized' , 'Final', 'Finally' ]\n",
        "for word in words:\n",
        "  print(word+'------>'+lematizer.lemmatize(word , pos ='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wf2wmiDLwy3",
        "outputId": "e81ae029-3be2-4d0c-8a88-25ff351c8d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating------>Eating\n",
            "Eat------>Eat\n",
            "Eaten------>Eaten\n",
            "History------>History\n",
            "Going------>Going\n",
            "Gone------>Gone\n",
            "Go------>Go\n",
            "Playing------>Playing\n",
            "Played------>Played\n",
            "Plays------>Plays\n",
            "Finalized------>Finalized\n",
            "Final------>Final\n",
            "Finally------>Finally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stopword , Parts of Speech , Named Entity Recognition\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "9vFZchkiMFk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "finyASQeyJtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dGWPYHyRNy",
        "outputId": "cb23d0c8-fdfd-4d06-9942-955ef5f8049d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS5toZtyyfuM",
        "outputId": "e6b6dd99-0d49-4cff-e71e-08aefe7e6ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "UgOvGgT1ysza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "_dclANU0zRC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Paragraph =  \"\"\"\n",
        "It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us. I thank you in the name of the most ancient order of monks in the world; I thank you in the name of the mother of religions; and I thank you in the name of the millions and millions of Hindu people of all classes and sects.\n",
        "\n",
        "My thanks, also, to some of the speakers on this platform who, referring to the delegates from the Orient, have told you that these men from far-off nations may well claim the honour of bearing to different lands the idea of toleration. I am proud to belong to a religion which has taught the world both tolerance and universal acceptance. We believe not only in universal toleration, but we accept all religions as true. I am proud to belong to a nation which has sheltered the persecuted and the refugees of all religions and all nations of the earth. I am proud to tell you that we have gathered in our bosom the purest remnant of the Israelites, who came to southern India and took refuge with us in the very year in which their holy temple was shattered to pieces by Roman tyranny. I am proud to belong to the religion which has sheltered and is still fostering the remnant of the grand Zoroastrian nation. I will quote to you, brethren, a few lines from a hymn which I remember to have repeated from my earliest boyhood, which is every day repeated by millions of human beings: ‘As the different streams having their sources in different places all mingle their water in the sea, so, O Lord, the different paths which men take through different tendencies, various though they appear, crooked or straight, all lead to Thee.’\n",
        "\n",
        "The present convention, which is one of the most august assemblies ever held, is in itself a vindication, a declaration to the world, of the wonderful doctrine preached in the Gita: ‘Whosoever comes to Me, through whatsoever form, I reach him; all men are struggling through paths which in the end lead to Me.’ Sectarianism, bigotry, and its horrible descendant, fanaticism, have long possessed this beautiful earth. They have filled the earth with violence, drenched it often and often with human blood, destroyed civilization, and sent whole nations to despair. Had it not been for these horrible demons, human society would be far more advanced than it is now. But their time is come; and I fervently hope that the bell that tolled this morning in honour of this convention may be the death-knell of all fanaticism, of all persecutions with the sword or with the pen, and of all uncharitable feelings between persons wending their way to the same goal.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "K2nJCfdOzV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QfjxPN1KTt",
        "outputId": "58a86426-384b-483b-fa20-35cab5572afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(Paragraph) # here the tokenization of the Paragraphs to Sentences is done."
      ],
      "metadata": {
        "id": "I0HhZyHv0ieY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64cLuHWz1AHd",
        "outputId": "50ee8940-9fe2-457c-9042-dd10111fd44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it fill heart joy unspeak rise respons warm cordial welcom given us .',\n",
              " 'i thank name ancient order monk world ; i thank name mother religion ; i thank name million million hindu peopl class sect .',\n",
              " 'my thank , also , speaker platform , refer deleg orient , told men far-off nation may well claim honour bear differ land idea toler .',\n",
              " 'i proud belong religion taught world toler univers accept .',\n",
              " 'we believ univers toler , accept religion true .',\n",
              " 'i proud belong nation shelter persecut refuge religion nation earth .',\n",
              " 'i proud tell gather bosom purest remnant israelit , came southern india took refug us year holi templ shatter piec roman tyranni .',\n",
              " 'i proud belong religion shelter still foster remnant grand zoroastrian nation .',\n",
              " 'i quot , brethren , line hymn i rememb repeat earliest boyhood , everi day repeat million human be : ‘ as differ stream sourc differ place mingl water sea , , o lord , differ path men take differ tendenc , variou though appear , crook straight , lead thee. ’ the present convent , one august assembl ever held , vindic , declar world , wonder doctrin preach gita : ‘ whosoev come me , whatsoev form , i reach ; men struggl path end lead me. ’ sectarian , bigotri , horribl descend , fanatic , long possess beauti earth .',\n",
              " 'they fill earth violenc , drench often often human blood , destroy civil , sent whole nation despair .',\n",
              " 'had horribl demon , human societi would far advanc .',\n",
              " 'but time come ; i fervent hope bell toll morn honour convent may death-knel fanatic , persecut sword pen , uncharit feel person wend way goal .']"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_15 = nltk.word_tokenize(Paragraph) # Here the tokenization of the Paragraphs to Words is done."
      ],
      "metadata": {
        "id": "Wqhc1kmLFUV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ei_yq_3Fx08",
        "outputId": "22e4b520-6a15-43e3-d1c2-848b63616783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " 'fills',\n",
              " 'my',\n",
              " 'heart',\n",
              " 'with',\n",
              " 'joy',\n",
              " 'unspeakable',\n",
              " 'to',\n",
              " 'rise',\n",
              " 'in',\n",
              " 'response',\n",
              " 'to',\n",
              " 'the',\n",
              " 'warm',\n",
              " 'and',\n",
              " 'cordial',\n",
              " 'welcome',\n",
              " 'which',\n",
              " 'you',\n",
              " 'have',\n",
              " 'given',\n",
              " 'us',\n",
              " '.',\n",
              " 'I',\n",
              " 'thank',\n",
              " 'you',\n",
              " 'in',\n",
              " 'the',\n",
              " 'name',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'ancient',\n",
              " 'order',\n",
              " 'of',\n",
              " 'monks',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ';',\n",
              " 'I',\n",
              " 'thank',\n",
              " 'you',\n",
              " 'in',\n",
              " 'the',\n",
              " 'name',\n",
              " 'of',\n",
              " 'the',\n",
              " 'mother',\n",
              " 'of',\n",
              " 'religions',\n",
              " ';',\n",
              " 'and',\n",
              " 'I',\n",
              " 'thank',\n",
              " 'you',\n",
              " 'in',\n",
              " 'the',\n",
              " 'name',\n",
              " 'of',\n",
              " 'the',\n",
              " 'millions',\n",
              " 'and',\n",
              " 'millions',\n",
              " 'of',\n",
              " 'Hindu',\n",
              " 'people',\n",
              " 'of',\n",
              " 'all',\n",
              " 'classes',\n",
              " 'and',\n",
              " 'sects',\n",
              " '.',\n",
              " 'My',\n",
              " 'thanks',\n",
              " ',',\n",
              " 'also',\n",
              " ',',\n",
              " 'to',\n",
              " 'some',\n",
              " 'of',\n",
              " 'the',\n",
              " 'speakers',\n",
              " 'on',\n",
              " 'this',\n",
              " 'platform',\n",
              " 'who',\n",
              " ',',\n",
              " 'referring',\n",
              " 'to',\n",
              " 'the',\n",
              " 'delegates',\n",
              " 'from',\n",
              " 'the',\n",
              " 'Orient',\n",
              " ',',\n",
              " 'have',\n",
              " 'told',\n",
              " 'you',\n",
              " 'that',\n",
              " 'these',\n",
              " 'men',\n",
              " 'from',\n",
              " 'far-off',\n",
              " 'nations',\n",
              " 'may',\n",
              " 'well',\n",
              " 'claim',\n",
              " 'the',\n",
              " 'honour',\n",
              " 'of',\n",
              " 'bearing',\n",
              " 'to',\n",
              " 'different',\n",
              " 'lands',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'of',\n",
              " 'toleration',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'proud',\n",
              " 'to',\n",
              " 'belong',\n",
              " 'to',\n",
              " 'a',\n",
              " 'religion',\n",
              " 'which',\n",
              " 'has',\n",
              " 'taught',\n",
              " 'the',\n",
              " 'world',\n",
              " 'both',\n",
              " 'tolerance',\n",
              " 'and',\n",
              " 'universal',\n",
              " 'acceptance',\n",
              " '.',\n",
              " 'We',\n",
              " 'believe',\n",
              " 'not',\n",
              " 'only',\n",
              " 'in',\n",
              " 'universal',\n",
              " 'toleration',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'accept',\n",
              " 'all',\n",
              " 'religions',\n",
              " 'as',\n",
              " 'true',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'proud',\n",
              " 'to',\n",
              " 'belong',\n",
              " 'to',\n",
              " 'a',\n",
              " 'nation',\n",
              " 'which',\n",
              " 'has',\n",
              " 'sheltered',\n",
              " 'the',\n",
              " 'persecuted',\n",
              " 'and',\n",
              " 'the',\n",
              " 'refugees',\n",
              " 'of',\n",
              " 'all',\n",
              " 'religions',\n",
              " 'and',\n",
              " 'all',\n",
              " 'nations',\n",
              " 'of',\n",
              " 'the',\n",
              " 'earth',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'proud',\n",
              " 'to',\n",
              " 'tell',\n",
              " 'you',\n",
              " 'that',\n",
              " 'we',\n",
              " 'have',\n",
              " 'gathered',\n",
              " 'in',\n",
              " 'our',\n",
              " 'bosom',\n",
              " 'the',\n",
              " 'purest',\n",
              " 'remnant',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Israelites',\n",
              " ',',\n",
              " 'who',\n",
              " 'came',\n",
              " 'to',\n",
              " 'southern',\n",
              " 'India',\n",
              " 'and',\n",
              " 'took',\n",
              " 'refuge',\n",
              " 'with',\n",
              " 'us',\n",
              " 'in',\n",
              " 'the',\n",
              " 'very',\n",
              " 'year',\n",
              " 'in',\n",
              " 'which',\n",
              " 'their',\n",
              " 'holy',\n",
              " 'temple',\n",
              " 'was',\n",
              " 'shattered',\n",
              " 'to',\n",
              " 'pieces',\n",
              " 'by',\n",
              " 'Roman',\n",
              " 'tyranny',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'proud',\n",
              " 'to',\n",
              " 'belong',\n",
              " 'to',\n",
              " 'the',\n",
              " 'religion',\n",
              " 'which',\n",
              " 'has',\n",
              " 'sheltered',\n",
              " 'and',\n",
              " 'is',\n",
              " 'still',\n",
              " 'fostering',\n",
              " 'the',\n",
              " 'remnant',\n",
              " 'of',\n",
              " 'the',\n",
              " 'grand',\n",
              " 'Zoroastrian',\n",
              " 'nation',\n",
              " '.',\n",
              " 'I',\n",
              " 'will',\n",
              " 'quote',\n",
              " 'to',\n",
              " 'you',\n",
              " ',',\n",
              " 'brethren',\n",
              " ',',\n",
              " 'a',\n",
              " 'few',\n",
              " 'lines',\n",
              " 'from',\n",
              " 'a',\n",
              " 'hymn',\n",
              " 'which',\n",
              " 'I',\n",
              " 'remember',\n",
              " 'to',\n",
              " 'have',\n",
              " 'repeated',\n",
              " 'from',\n",
              " 'my',\n",
              " 'earliest',\n",
              " 'boyhood',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'every',\n",
              " 'day',\n",
              " 'repeated',\n",
              " 'by',\n",
              " 'millions',\n",
              " 'of',\n",
              " 'human',\n",
              " 'beings',\n",
              " ':',\n",
              " '‘',\n",
              " 'As',\n",
              " 'the',\n",
              " 'different',\n",
              " 'streams',\n",
              " 'having',\n",
              " 'their',\n",
              " 'sources',\n",
              " 'in',\n",
              " 'different',\n",
              " 'places',\n",
              " 'all',\n",
              " 'mingle',\n",
              " 'their',\n",
              " 'water',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sea',\n",
              " ',',\n",
              " 'so',\n",
              " ',',\n",
              " 'O',\n",
              " 'Lord',\n",
              " ',',\n",
              " 'the',\n",
              " 'different',\n",
              " 'paths',\n",
              " 'which',\n",
              " 'men',\n",
              " 'take',\n",
              " 'through',\n",
              " 'different',\n",
              " 'tendencies',\n",
              " ',',\n",
              " 'various',\n",
              " 'though',\n",
              " 'they',\n",
              " 'appear',\n",
              " ',',\n",
              " 'crooked',\n",
              " 'or',\n",
              " 'straight',\n",
              " ',',\n",
              " 'all',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'Thee.',\n",
              " '’',\n",
              " 'The',\n",
              " 'present',\n",
              " 'convention',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'august',\n",
              " 'assemblies',\n",
              " 'ever',\n",
              " 'held',\n",
              " ',',\n",
              " 'is',\n",
              " 'in',\n",
              " 'itself',\n",
              " 'a',\n",
              " 'vindication',\n",
              " ',',\n",
              " 'a',\n",
              " 'declaration',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'of',\n",
              " 'the',\n",
              " 'wonderful',\n",
              " 'doctrine',\n",
              " 'preached',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Gita',\n",
              " ':',\n",
              " '‘',\n",
              " 'Whosoever',\n",
              " 'comes',\n",
              " 'to',\n",
              " 'Me',\n",
              " ',',\n",
              " 'through',\n",
              " 'whatsoever',\n",
              " 'form',\n",
              " ',',\n",
              " 'I',\n",
              " 'reach',\n",
              " 'him',\n",
              " ';',\n",
              " 'all',\n",
              " 'men',\n",
              " 'are',\n",
              " 'struggling',\n",
              " 'through',\n",
              " 'paths',\n",
              " 'which',\n",
              " 'in',\n",
              " 'the',\n",
              " 'end',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'Me.',\n",
              " '’',\n",
              " 'Sectarianism',\n",
              " ',',\n",
              " 'bigotry',\n",
              " ',',\n",
              " 'and',\n",
              " 'its',\n",
              " 'horrible',\n",
              " 'descendant',\n",
              " ',',\n",
              " 'fanaticism',\n",
              " ',',\n",
              " 'have',\n",
              " 'long',\n",
              " 'possessed',\n",
              " 'this',\n",
              " 'beautiful',\n",
              " 'earth',\n",
              " '.',\n",
              " 'They',\n",
              " 'have',\n",
              " 'filled',\n",
              " 'the',\n",
              " 'earth',\n",
              " 'with',\n",
              " 'violence',\n",
              " ',',\n",
              " 'drenched',\n",
              " 'it',\n",
              " 'often',\n",
              " 'and',\n",
              " 'often',\n",
              " 'with',\n",
              " 'human',\n",
              " 'blood',\n",
              " ',',\n",
              " 'destroyed',\n",
              " 'civilization',\n",
              " ',',\n",
              " 'and',\n",
              " 'sent',\n",
              " 'whole',\n",
              " 'nations',\n",
              " 'to',\n",
              " 'despair',\n",
              " '.',\n",
              " 'Had',\n",
              " 'it',\n",
              " 'not',\n",
              " 'been',\n",
              " 'for',\n",
              " 'these',\n",
              " 'horrible',\n",
              " 'demons',\n",
              " ',',\n",
              " 'human',\n",
              " 'society',\n",
              " 'would',\n",
              " 'be',\n",
              " 'far',\n",
              " 'more',\n",
              " 'advanced',\n",
              " 'than',\n",
              " 'it',\n",
              " 'is',\n",
              " 'now',\n",
              " '.',\n",
              " 'But',\n",
              " 'their',\n",
              " 'time',\n",
              " 'is',\n",
              " 'come',\n",
              " ';',\n",
              " 'and',\n",
              " 'I',\n",
              " 'fervently',\n",
              " 'hope',\n",
              " 'that',\n",
              " 'the',\n",
              " 'bell',\n",
              " 'that',\n",
              " 'tolled',\n",
              " 'this',\n",
              " 'morning',\n",
              " 'in',\n",
              " 'honour',\n",
              " 'of',\n",
              " 'this',\n",
              " 'convention',\n",
              " 'may',\n",
              " 'be',\n",
              " 'the',\n",
              " 'death-knell',\n",
              " 'of',\n",
              " 'all',\n",
              " 'fanaticism',\n",
              " ',',\n",
              " 'of',\n",
              " 'all',\n",
              " 'persecutions',\n",
              " 'with',\n",
              " 'the',\n",
              " 'sword',\n",
              " 'or',\n",
              " 'with',\n",
              " 'the',\n",
              " 'pen',\n",
              " ',',\n",
              " 'and',\n",
              " 'of',\n",
              " 'all',\n",
              " 'uncharitable',\n",
              " 'feelings',\n",
              " 'between',\n",
              " 'persons',\n",
              " 'wending',\n",
              " 'their',\n",
              " 'way',\n",
              " 'to',\n",
              " 'the',\n",
              " 'same',\n",
              " 'goal',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words) # converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "J7FZcxsE1m-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEZTep0L4QAB",
        "outputId": "63008534-d9f3-4f75-df3e-96660d109809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it fill heart joy unspeak rise respons warm cordial welcom given us .',\n",
              " 'i thank name ancient order monk world ; i thank name mother religion ; i thank name million million hindu peopl class sect .',\n",
              " 'my thank , also , speaker platform , refer deleg orient , told men far-off nation may well claim honour bear differ land idea toler .',\n",
              " 'i proud belong religion taught world toler univers accept .',\n",
              " 'we believ univers toler , accept religion true .',\n",
              " 'i proud belong nation shelter persecut refuge religion nation earth .',\n",
              " 'i proud tell gather bosom purest remnant israelit , came southern india took refug us year holi templ shatter piec roman tyranni .',\n",
              " 'i proud belong religion shelter still foster remnant grand zoroastrian nation .',\n",
              " 'i quot , brethren , line hymn i rememb repeat earliest boyhood , everi day repeat million human be : ‘ as differ stream sourc differ place mingl water sea , , o lord , differ path men take differ tendenc , variou though appear , crook straight , lead thee. ’ the present convent , one august assembl ever held , vindic , declar world , wonder doctrin preach gita : ‘ whosoev come me , whatsoev form , i reach ; men struggl path end lead me. ’ sectarian , bigotri , horribl descend , fanatic , long possess beauti earth .',\n",
              " 'they fill earth violenc , drench often often human blood , destroy civil , sent whole nation despair .',\n",
              " 'had horribl demon , human societi would far advanc .',\n",
              " 'but time come ; i fervent hope bell toll morn honour convent may death-knel fanatic , persecut sword pen , uncharit feel person wend way goal .']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now doing the same above thing with the snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "DpItugiV5R3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_14 = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "HZXtEEti6KjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences_1 = ' '.join(words) # converting all the list  of words into sentences"
      ],
      "metadata": {
        "id": "YCUH1XWL6Rce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UqWSu81R9XYu",
        "outputId": "27deda94-51b2-4188-917f-81189943bb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time come ; fervent hope bell toll morn honour convent may death-knel fanat , persecut sword pen , uncharit feel person wend way goal .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Lematization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer_1 = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "LKtrkTQe9anJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer_1.lemmatize(word , pos='v') for word in words if word not in set(stopwords.words('english'))] ## here also we can use word.lower() , to lowercase the words of the sentences.\n",
        "  sentences_12= ' '.join(words)"
      ],
      "metadata": {
        "id": "HzjoufizAZhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f9aP6sOMBTg2",
        "outputId": "15beed63-5960-4bbb-acd9-f28856ac8f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time come ; fervent hope bell toll morn honour convent may death-knel fanatic , persecut sword pen , uncharit feel person wend way goal .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3TGvFv6nBVOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}